{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLXy8ti6E1If",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "1b5d3e92-b68e-494d-bea6-84cd0f3083f1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sklearn\n",
        "import operator\n",
        "import random\n",
        "import math\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "pos_train = pd.read_csv('https://raw.githubusercontent.com/PuruTiwari/CMT307-C1978887/master/datasets_coursework1/IMDb/train/imdb_train_pos.txt', sep=\"\\n\", header=None)\n",
        "neg_train = pd.read_csv('https://raw.githubusercontent.com/PuruTiwari/CMT307-C1978887/master/datasets_coursework1/IMDb/train/imdb_train_neg.txt', sep=\"\\n\", header=None)\n",
        "pos_train = pos_train.iloc[:,0].as_matrix()\n",
        "neg_train = neg_train.iloc[:,0].as_matrix()\n",
        "\n",
        "pos_dev = pd.read_csv('https://raw.githubusercontent.com/PuruTiwari/CMT307-C1978887/master/datasets_coursework1/IMDb/dev/imdb_dev_pos.txt', sep=\"\\n\", header=None)\n",
        "neg_dev = pd.read_csv('https://raw.githubusercontent.com/PuruTiwari/CMT307-C1978887/master/datasets_coursework1/IMDb/dev/imdb_dev_neg.txt', sep=\"\\n\", header=None)\n",
        "pos_dev = pos_dev.iloc[:,0].as_matrix()\n",
        "neg_dev = neg_dev.iloc[:,0].as_matrix()\n",
        "\n",
        "pos_test = pd.read_csv('https://raw.githubusercontent.com/PuruTiwari/CMT307-C1978887/master/datasets_coursework1/IMDb/test/imdb_test_pos.txt', sep=\"\\n\", header=None)\n",
        "neg_test = pd.read_csv('https://raw.githubusercontent.com/PuruTiwari/CMT307-C1978887/master/datasets_coursework1/IMDb/test/imdb_test_neg.txt', sep=\"\\n\", header=None)\n",
        "pos_test = pos_test.iloc[:,0].as_matrix()\n",
        "neg_test = neg_test.iloc[:,0].as_matrix()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQjlZtTmME2N",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing Data using lemmatizer and stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgq_uSjUGVnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "3c34da6a-3404-402e-cbbe-c4fda2a5d64e"
      },
      "source": [
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def get_list_tokens(string):\n",
        "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
        "  list_tokens=[]\n",
        "  for sentence in sentence_split:\n",
        "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
        "    for token in list_tokens_sentence:\n",
        "      list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
        "  return list_tokens\n",
        "\n",
        "# get the english stopwords list from nltk\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "# add more words to the stopword list\n",
        "stopwords.add(\"/\")\n",
        "stopwords.add(\".\")\n",
        "stopwords.add(\">\")\n",
        "stopwords.add(\"''\")\n",
        "stopwords.add(\",\")\n",
        "stopwords.add(\"--\")\n",
        "stopwords.add(\"``\")\n",
        "stopwords.add(\"<\")\n",
        "stopwords.add(\"br\")\n",
        "stopwords.add(\"'s\")\n",
        "stopwords.add(\")\")\n",
        "stopwords.add(\"(\")\n",
        "\n",
        "dict_word_frequency = {}\n",
        "\n",
        "for pos_review in pos_train:\n",
        "  sentence_tokens = get_list_tokens(pos_review)\n",
        "  for word in sentence_tokens:\n",
        "    if word in stopwords: continue\n",
        "    if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
        "    else: dict_word_frequency[word]+=1\n",
        "\n",
        "for neg_review in neg_train:\n",
        "  sentence_tokens = get_list_tokens(neg_review)\n",
        "  for word in sentence_tokens:\n",
        "    if word in stopwords: continue\n",
        "    if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
        "    else: dict_word_frequency[word]+=1\n",
        "\n",
        "sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:1000]\n",
        "i=0\n",
        "for word, frequency in sorted_list[:15]:\n",
        "  i+=1\n",
        "  print (str(i)+\". \"+word+\" - \"+str(frequency))\n",
        "\n",
        "vocabulary=[]\n",
        "for word,frequency in sorted_list:\n",
        "  vocabulary.append(word)\n",
        "def get_vector_text(list_vocab,string):\n",
        "  vector_text=np.zeros(len(list_vocab))\n",
        "  list_tokens_string=get_list_tokens(string)\n",
        "  for i, word in enumerate(list_vocab):\n",
        "    if word in list_tokens_string:\n",
        "      vector_text[i]=list_tokens_string.count(word)\n",
        "  return vector_text\n",
        "\n",
        "\n",
        "X_train=[]\n",
        "Y_train=[]\n",
        "for pos_review in pos_train:\n",
        "  vector_pos_review=get_vector_text(vocabulary,pos_review)\n",
        "  X_train.append(vector_pos_review)\n",
        "  Y_train.append(1)\n",
        "for neg_review in neg_train:\n",
        "  vector_neg_review=get_vector_text(vocabulary,neg_review)\n",
        "  X_train.append(vector_neg_review)\n",
        "  Y_train.append(0)\n",
        "\n",
        "X_dev=[]\n",
        "Y_dev=[]\n",
        "for pos_review in pos_dev:\n",
        "  vector_pos_review=get_vector_text(vocabulary,pos_review)\n",
        "  X_dev.append(vector_pos_review)\n",
        "  Y_dev.append(1)\n",
        "for neg_review in neg_dev:\n",
        "  vector_neg_review=get_vector_text(vocabulary,neg_review)\n",
        "  X_dev.append(vector_neg_review)\n",
        "  Y_dev.append(0)\n",
        "\n",
        "X_test=[]\n",
        "Y_test=[]\n",
        "for pos_review in pos_test:\n",
        "  vector_pos_review=get_vector_text(vocabulary,pos_review)\n",
        "  X_test.append(vector_pos_review)\n",
        "  Y_test.append(1)\n",
        "for neg_review in neg_test:\n",
        "  vector_neg_review=get_vector_text(vocabulary,neg_review)\n",
        "  X_test.append(vector_neg_review)\n",
        "  Y_test.append(0)\n",
        "\n",
        "TF_train = []\n",
        "for f_vector in X_train:\n",
        "  TF_train.append(f_vector/sum(f_vector))\n",
        "\n",
        "X_train_temp = np.asarray(X_train)\n",
        "IDF_vec = []\n",
        "for i in range(X_train_temp.shape[1]):\n",
        "\n",
        "  count_temp = 0\n",
        "  for j in range(X_train_temp.shape[0]):\n",
        "    if X_train_temp[j,i] != 0:\n",
        "      count_temp += 1\n",
        "\n",
        "  IDF_vec.append(math.log(X_train_temp.shape[0]/(count_temp+1)))\n",
        "\n",
        "TF_IDF_train = []\n",
        "for TF_vector in TF_train:\n",
        "  TF_IDF_train.append(TF_vector * IDF_vec)\n",
        "\n",
        "TF_dev = []\n",
        "for f_vector in X_dev:\n",
        "  TF_dev.append(f_vector/sum(f_vector))\n",
        "\n",
        "TF_IDF_dev = []\n",
        "for TF_vector in TF_dev:\n",
        "  TF_IDF_dev.append(TF_vector * IDF_vec)\n",
        "\n",
        "TF_test = []\n",
        "for f_vector in X_test:\n",
        "  TF_test.append(f_vector/sum(f_vector))\n",
        "\n",
        "TF_IDF_test = []\n",
        "for TF_vector in TF_test:\n",
        "  TF_IDF_test.append(TF_vector * IDF_vec)  "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. movie - 29647\n",
            "2. wa - 29577\n",
            "3. film - 26929\n",
            "4. n't - 19639\n",
            "5. one - 15987\n",
            "6. ! - 14847\n",
            "7. like - 11876\n",
            "8. ha - 9893\n",
            "9. ? - 9593\n",
            "10. time - 8589\n",
            "11. good - 8376\n",
            "12. character - 8318\n",
            "13. would - 7867\n",
            "14. ... - 7722\n",
            "15. even - 7321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4hHT22FDPq7",
        "colab_type": "text"
      },
      "source": [
        "Feature selection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6peS7mZOHaX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "b66e9166-e331-4420-c769-b90293a3f369"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "train_drop_stopwords = []\n",
        "for pos_review in pos_train:\n",
        "  sentence_tokens = get_list_tokens(pos_review)\n",
        "  new_sentence = ''\n",
        "  for i, word in enumerate(sentence_tokens):\n",
        "    if word in stopwords: continue\n",
        "    if word not in vocabulary: continue\n",
        "    if i == 0:\n",
        "      new_sentence = new_sentence + word\n",
        "    else:\n",
        "      new_sentence = new_sentence + ' ' + word\n",
        "  train_drop_stopwords.append(new_sentence) \n",
        "\n",
        "for neg_review in neg_train:\n",
        "  sentence_tokens = get_list_tokens(neg_review)\n",
        "  new_sentence = ''\n",
        "  for i, word in enumerate(sentence_tokens):\n",
        "    if word in stopwords: continue\n",
        "    if word not in vocabulary: continue\n",
        "    if i == 0:\n",
        "      new_sentence = new_sentence + word\n",
        "    else:\n",
        "      new_sentence = new_sentence + ' ' + word\n",
        "  train_drop_stopwords.append(new_sentence)\n",
        "\n",
        "twoGram = CountVectorizer(min_df=1, ngram_range=(2,2))\n",
        "twoGram_train = twoGram.fit_transform(train_drop_stopwords)\n",
        "dev_drop_stopwords = []\n",
        "for pos_review in pos_dev:\n",
        "  sentence_tokens = get_list_tokens(pos_review)\n",
        "  new_sentence = ''\n",
        "  for i, word in enumerate(sentence_tokens):\n",
        "    if word in stopwords: continue\n",
        "    if word not in vocabulary: continue\n",
        "    if i == 0:\n",
        "      new_sentence = new_sentence + word\n",
        "    else:\n",
        "      new_sentence = new_sentence + ' ' + word\n",
        "  dev_drop_stopwords.append(new_sentence)\n",
        "\n",
        "for neg_review in neg_dev:\n",
        "  sentence_tokens = get_list_tokens(neg_review)\n",
        "  new_sentence = ''\n",
        "  for i, word in enumerate(sentence_tokens):\n",
        "    if word in stopwords: continue\n",
        "    if word not in vocabulary: continue\n",
        "    if i == 0:\n",
        "      new_sentence = new_sentence + word\n",
        "    else:\n",
        "      new_sentence = new_sentence + ' ' + word\n",
        "  dev_drop_stopwords.append(new_sentence)\n",
        "\n",
        "twoGram_dev = twoGram.transform(dev_drop_stopwords)\n",
        "\n",
        "test_drop_stopwords = []\n",
        "for pos_review in pos_test:\n",
        "  sentence_tokens = get_list_tokens(pos_review)\n",
        "  new_sentence = ''\n",
        "  for i, word in enumerate(sentence_tokens):\n",
        "    if word in stopwords: continue\n",
        "    if word not in vocabulary: continue\n",
        "    if i == 0:\n",
        "      new_sentence = new_sentence + word\n",
        "    else:\n",
        "      new_sentence = new_sentence + ' ' + word\n",
        "  test_drop_stopwords.append(new_sentence)\n",
        "\n",
        "for neg_review in neg_test:\n",
        "  sentence_tokens = get_list_tokens(neg_review)\n",
        "  new_sentence = ''\n",
        "  for i, word in enumerate(sentence_tokens):\n",
        "    if word in stopwords: continue\n",
        "    if word not in vocabulary: continue\n",
        "    if i == 0:\n",
        "      new_sentence = new_sentence + word\n",
        "    else:\n",
        "      new_sentence = new_sentence + ' ' + word\n",
        "  test_drop_stopwords.append(new_sentence)\n",
        "\n",
        "twoGram_test = twoGram.transform(test_drop_stopwords)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "W2V_base = Word2Vec(train_drop_stopwords, min_count=5, size=500, workers=4)\n",
        "W2V_train = []\n",
        "for sentence in train_drop_stopwords:\n",
        "  temp_vector = np.zeros(500)\n",
        "  count = 0\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      temp_vector += W2V_base[word]\n",
        "      count += 1\n",
        "    except:\n",
        "      pass\n",
        "  W2V_train.append(temp_vector/count)\n",
        "\n",
        "W2V_dev = []\n",
        "for sentence in dev_drop_stopwords:\n",
        "  temp_vector = np.zeros(500)\n",
        "  count = 0\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      temp_vector += W2V_base[word]\n",
        "      count += 1\n",
        "    except:\n",
        "      pass\n",
        "  W2V_dev.append(temp_vector/count)\n",
        "\n",
        "\n",
        "W2V_test = []\n",
        "for sentence in test_drop_stopwords:\n",
        "  temp_vector = np.zeros(500)\n",
        "  count = 0\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      temp_vector += W2V_base[word]\n",
        "      count += 1\n",
        "    except:\n",
        "      pass\n",
        "  W2V_test.append(temp_vector/count)\n",
        "\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "TF_IDF_train_fix = np.asarray(TF_IDF_train)\n",
        "TF_IDF_dev_fix = np.asarray(TF_IDF_dev)\n",
        "TF_IDF_test_fix = np.asarray(TF_IDF_test)\n",
        "\n",
        "Y_train_fix = np.asarray(Y_train)\n",
        "Y_dev_fix = np.asarray(Y_dev)\n",
        "Y_test_fix = np.asarray(Y_test)\n",
        "\n",
        "TF_IDF_select = SelectKBest(chi2, k=500).fit(TF_IDF_train_fix, Y_train_fix)\n",
        "TF_IDF_train_selected = TF_IDF_select.transform(TF_IDF_train_fix)\n",
        "TF_IDF_dev_selected = TF_IDF_select.transform(TF_IDF_dev_fix)\n",
        "TF_IDF_test_selected = TF_IDF_select.transform(TF_IDF_test_fix)\n",
        "\n",
        "print (\"Size original training matrix: \"+str(TF_IDF_train_fix.shape))\n",
        "print (\"Size new training matrix: \"+str(TF_IDF_train_selected.shape))\n",
        "\n",
        "twoGram_select = SelectKBest(chi2, k=1000).fit(twoGram_train, Y_train_fix)\n",
        "twoGram_train_selected = twoGram_select.transform(twoGram_train)\n",
        "twoGram_dev_selected = twoGram_select.transform(twoGram_dev)\n",
        "twoGram_test_selected = twoGram_select.transform(twoGram_test)\n",
        "\n",
        "print (\"Size original training matrix: \"+str(twoGram_train.shape))\n",
        "print (\"Size new training matrix: \"+str(twoGram_train_selected.shape))\n",
        "\n",
        "W2V_train_fix = np.asarray(W2V_train)\n",
        "W2V_dev_fix = np.asarray(W2V_dev)\n",
        "W2V_test_fix = np.asarray(W2V_test)\n",
        "\n",
        "W2V_select = SelectKBest(f_classif, k=300).fit(W2V_train_fix, Y_train_fix)\n",
        "W2V_train_selected = W2V_select.transform(W2V_train_fix)\n",
        "W2V_dev_selected = W2V_select.transform(W2V_dev_fix)\n",
        "W2V_test_selected = W2V_select.transform(W2V_test_fix)\n",
        "\n",
        "print (\"Size original training matrix: \"+str(W2V_train_fix.shape))\n",
        "print (\"Size new training matrix: \"+str(W2V_train_selected.shape))\n",
        "\n",
        "twoGram_train_selected = np.asarray(twoGram_train_selected.todense())\n",
        "twoGram_dev_selected = np.asarray(twoGram_dev_selected.todense())\n",
        "twoGram_test_selected = np.asarray(twoGram_test_selected.todense())\n",
        "\n",
        "COM_train = np.column_stack((TF_IDF_train_selected, twoGram_train_selected, W2V_train_selected))\n",
        "COM_dev = np.column_stack((TF_IDF_dev_selected, twoGram_dev_selected, W2V_dev_selected))\n",
        "COM_test = np.column_stack((TF_IDF_test_selected, twoGram_test_selected, W2V_test_selected))\n",
        "\n",
        "COM_select = SelectKBest(f_classif, k=1000).fit(COM_train, Y_train_fix)\n",
        "COM_train_selected = COM_select.transform(COM_train)\n",
        "COM_dev_selected = COM_select.transform(COM_dev)\n",
        "COM_test_selected = COM_select.transform(COM_test)\n",
        "\n",
        "print (\"Size original training matrix: \"+str(COM_train.shape))\n",
        "print (\"Size new training matrix: \"+str(COM_train_selected.shape))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:93: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:105: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:118: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Size original training matrix: (15000, 1000)\n",
            "Size new training matrix: (15000, 500)\n",
            "Size original training matrix: (15000, 325515)\n",
            "Size new training matrix: (15000, 1000)\n",
            "Size original training matrix: (15000, 500)\n",
            "Size new training matrix: (15000, 300)\n",
            "Size original training matrix: (15000, 1800)\n",
            "Size new training matrix: (15000, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYzdJxRXDV1P",
        "colab_type": "text"
      },
      "source": [
        "Evaluating Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpJGlKyUIJ0B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "180a8598-6703-4d9c-e17a-ea26db7fa80d"
      },
      "source": [
        "\n",
        "svm_1st = sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
        "svm_1st.fit(COM_train_selected, Y_train_fix)\n",
        "Y_dev_pred = svm_1st.predict(COM_dev_selected)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
        "\n",
        "precision = precision_score(Y_dev_fix, Y_dev_pred, average='macro')\n",
        "recall = recall_score(Y_dev_fix, Y_dev_pred, average='macro')\n",
        "f1 = f1_score(Y_dev_fix, Y_dev_pred, average='macro')\n",
        "accuracy = accuracy_score(Y_dev_fix, Y_dev_pred)\n",
        "\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)\n",
        "print(accuracy)\n",
        "\n",
        "COM_train_new = np.column_stack((TF_IDF_train, twoGram_train_selected, W2V_train))\n",
        "COM_dev_new = np.column_stack((TF_IDF_dev, twoGram_dev_selected, W2V_dev))\n",
        "COM_test_new = np.column_stack((TF_IDF_test, twoGram_test_selected, W2V_test))\n",
        "\n",
        "COM_select_new = SelectKBest(f_classif, k=1000).fit(COM_train_new, Y_train_fix)\n",
        "COM_train_selected_new = COM_select_new.transform(COM_train_new)\n",
        "COM_dev_selected_new = COM_select_new.transform(COM_dev_new)\n",
        "COM_test_selected_new = COM_select_new.transform(COM_test_new)\n",
        "\n",
        "print (\"Size original training matrix: \"+str(COM_train_new.shape))\n",
        "print (\"Size new training matrix: \"+str(COM_test_selected_new.shape))\n",
        "svm_2nd = sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
        "svm_2nd.fit(COM_train_selected_new, Y_train_fix)\n",
        "\n",
        "Y_dev_pred = svm_2nd.predict(COM_dev_selected_new)\n",
        "\n",
        "precision = precision_score(Y_dev_fix, Y_dev_pred, average='macro')\n",
        "recall = recall_score(Y_dev_fix, Y_dev_pred, average='macro')\n",
        "f1 = f1_score(Y_dev_fix, Y_dev_pred, average='macro')\n",
        "accuracy = accuracy_score(Y_dev_fix, Y_dev_pred)\n",
        "\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)\n",
        "print(accuracy)\n",
        "\n",
        "Y_test_pred = svm_1st.predict(COM_test_selected)\n",
        "\n",
        "precision = precision_score(Y_test_fix, Y_test_pred, average='macro')\n",
        "recall = recall_score(Y_test_fix, Y_test_pred, average='macro')\n",
        "f1 = f1_score(Y_test_fix, Y_test_pred, average='macro')\n",
        "accuracy = accuracy_score(Y_test_fix, Y_test_pred)\n",
        "\n",
        "print('Precision - ',precision)\n",
        "print('Recall - ',recall)\n",
        "print('F1 - ',f1)\n",
        "print('Accuracy ',accuracy)\n",
        "\n",
        "Y_test_pred = svm_2nd.predict(COM_test_selected_new)\n",
        "\n",
        "precision = precision_score(Y_test_fix, Y_test_pred, average='macro')\n",
        "recall = recall_score(Y_test_fix, Y_test_pred, average='macro')\n",
        "f1 = f1_score(Y_test_fix, Y_test_pred, average='macro')\n",
        "accuracy = accuracy_score(Y_test_fix, Y_test_pred)\n",
        "\n",
        "print('Precision - ',precision)\n",
        "print('Recall - ',recall)\n",
        "print('F1 - ',f1)\n",
        "print('Accuracy ',accuracy)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8366259745608582\n",
            "0.8358417300352851\n",
            "0.835871323117324\n",
            "0.836\n",
            "Size original training matrix: (15000, 2500)\n",
            "Size new training matrix: (5000, 1000)\n",
            "0.8386029706945002\n",
            "0.8374014908932879\n",
            "0.8374164496750252\n",
            "0.8376\n",
            "Precision -  0.8392383302377096\n",
            "Recall -  0.8376138940182231\n",
            "F1 -  0.8374075946512065\n",
            "Accuracy  0.8376\n",
            "Precision -  0.835803264604811\n",
            "Recall -  0.8342138134742101\n",
            "F1 -  0.8340058465984154\n",
            "Accuracy  0.8342\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}